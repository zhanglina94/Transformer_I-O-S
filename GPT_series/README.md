# GPT 

### GPT 

2018_Improving Language Understanding by Generative Pre-Training

PaperÔºöradford2018improving.pdf (ubc.ca)
CodeÔºö
GitHub - huggingface/pytorch-openai-transformer-lm: üê•A PyTorch implementation of OpenAI's finetuned transformer language model with a script to import the weights pre-trained by OpenAI


### GPT2

2019_Language Models are Unsupervised Multitask Learners

Paper:Language Models are Unsupervised Multitask Learners (d4mucfpksywv.cloudfront.net)
Code:GitHub - openai/gpt-2: Code for the paper "Language Models are Unsupervised Multitask Learners"

### GPT3

2020_Language Models are Few-Shot Learners

Paper:1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf (neurips.cc)
Code :GitHub - EleutherAI/gpt-neox: An implementation of model parallel autoregressive transformers on GPUs, based on the DeepSpeed library.
